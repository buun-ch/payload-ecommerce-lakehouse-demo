# Payload Ecommerce Lakehouse Demo

A demonstration of integrating a Next.js ecommerce application with an open-source lakehouse architecture for analytics and business intelligence.

## Overview

This project showcases how to build a modern data stack by combining:

- **Next.js Application**: Full-featured ecommerce site built with Payload CMS
- **Data Extraction**: Automated data ingestion with dlt (data load tool)
- **Data Transformation**: Analytics-ready models with dbt (data build tool)
- **Lakehouse Storage**: Apache Iceberg tables via Lakekeeper REST Catalog
- **Query Engine**: Trino for distributed SQL queries
- **BI & Visualization**: Metabase for dashboards and reporting

## Architecture

```plain
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js + Payload â”‚ Ecommerce Application
â”‚   (PostgreSQL)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     dlt      â”‚ Data Ingestion
    â”‚  (Python)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Iceberg REST Catalogâ”‚ Lakehouse Storage
â”‚   (Lakekeeper)       â”‚
â”‚   â†“                  â”‚
â”‚ S3/MinIO (Parquet)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     dbt      â”‚ Data Transformation
    â”‚   + Trino    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Metabase   â”‚ Analytics & BI
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

### ğŸ›’ Ecommerce Application

Built with [Payload CMS ecommerce template](./README-payload.md):

- Product catalog with variants and categories
- Shopping cart and checkout flow
- Order management
- Customer accounts
- Stripe payment integration
- Admin panel for content management

### ğŸ“Š Analytics Pipeline

**Data Ingestion (dlt)**:

- Incremental data extraction from Payload CMS API
- Change Data Capture (CDC) via `updated_since` parameter
- Direct writes to Iceberg tables
- Schema evolution support

**Data Transformation (dbt)**:

- Star schema design (fact & dimension tables)
- JSON parsing with Trino native functions
- Incremental models for efficient updates
- Data quality tests
- See: [data/transformation/README.md](./data/transformation/README.md)

**Analytics**:

- Pre-built SQL queries for common analyses
- Customer segmentation (RFM analysis)
- Product performance tracking
- Cohort retention analysis
- See: [docs/analysis_queries.md](./docs/analysis_queries.md)

## Getting Started

### Prerequisites

- [mise](https://mise.jdx.dev/) - Development tool version manager

**Note**: Node.js, Python, pnpm, and just are managed by mise and will be installed automatically.

### Quick Start

1. **Clone and setup the application:**

   ```bash
   git clone <repository-url>
   cd payload-ecommerce-lakehouse-demo

   # Trust mise configuration to install development tools
   mise trust
   mise install

   # View available tasks
   just

   # Install dependencies
   pnpm install

   # Setup environment
   cp .env.example .env
   # Edit .env with your configuration

   # Start the app
   pnpm dev
   ```

   **Note**: This project uses [just](https://github.com/casey/just) as a command runner. Run `just` without arguments to see all available recipes.

   See [README-payload.md](./README-payload.md) for detailed application setup.

2. **Seed demo data:**

   ```bash
   # Generate sample ecommerce data
   just seed
   ```

   See [docs/seed.md](./docs/seed.md) for seeding options and data volume.

3. **Ingest data to lakehouse:**

   ```bash
   cd data/ingestion

   # Setup environment
   cp env.local.example .env.local
   # Edit .env.local with your configuration

   # Install dependencies
   pip install -r requirements.txt

   # Run ingestion
   just dlt::op-run
   ```

4. **Transform data with dbt:**

   ```bash
   cd data/transformation

   # Setup environment
   cp env.local.example .env.local
   # Edit .env.local with your configuration

   # Install dependencies
   pip install -r requirements.txt

   # Setup dbt profile
   mkdir -p ~/.dbt
   cp profiles.yml ~/.dbt/profiles.yml

   # Run transformations
   just dbt::op-run --target=prod
   ```

5. **Query with Trino/Metabase:**

   Sample queries available in [docs/analysis_queries.md](./docs/analysis_queries.md)

## Project Structure

```plain
.
â”œâ”€â”€ src/                          # Next.js application source
â”‚   â”œâ”€â”€ app/                      # App router pages
â”‚   â”œâ”€â”€ collections/              # Payload CMS collections
â”‚   â”œâ”€â”€ components/               # React components
â”‚   â””â”€â”€ plugins/                  # Payload plugins (ecommerce, SEO, etc)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ingestion/                # dlt pipeline for data extraction
â”‚   â””â”€â”€ transformation/           # dbt models for analytics
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ analysis_queries.md       # Sample SQL queries for analytics
â”‚   â””â”€â”€ seed.md                   # Data seeding guide
â”œâ”€â”€ public/                       # Static assets
â”œâ”€â”€ tests/                        # Integration & E2E tests
â””â”€â”€ README-payload.md             # Original Payload CMS documentation
```

## buun-stack Integration

This project is designed to work optimally with [buun-stack](https://github.com/buun-ch/buun-stack), which provides:

- **Kubernetes cluster** with Telepresence for local development
- **Lakekeeper** (Iceberg REST Catalog)
- **Trino** distributed query engine
- **MinIO** S3-compatible object storage
- **Metabase** for BI and visualization
- **Managed infrastructure** for the entire data stack

## Development Workflow

### Typical Development Cycle

1. **Develop features** in the Next.js app
2. **Seed data** to test with realistic volumes
3. **Ingest data** to the lakehouse (runs incrementally)
4. **Transform data** with dbt (dev environment)
5. **Query & visualize** in Metabase
6. **Deploy to production** (prod target)

### Data Refresh

```bash
# Full refresh (rebuild all data)
just dlt::op-run --full-refresh
just dbt::op-run --target=prod --full-refresh

# Incremental refresh (new/updated records only)
just dlt::op-run                    # Detects changes via updated_since
just dbt::op-run --target=prod      # Incremental models only
```

## Technologies

### Application Stack

- **Frontend**: Next.js 15, React 19, TailwindCSS
- **CMS**: Payload CMS 3.0
- **Database**: PostgreSQL

### Data Stack

- **Ingestion**: dlt (data load tool)
- **Storage**: Apache Iceberg (open table format)
- **Catalog**: Lakekeeper (Iceberg REST Catalog)
- **Query**: Trino (distributed SQL engine)
- **Transformation**: dbt (data build tool)
- **BI**: Metabase

### Infrastructure

- **Kubernetes**: Container orchestration
- **Telepresence**: Local development with remote services
- **MinIO**: S3-compatible object storage
- **1Password CLI**: Secrets management

### Development Tools

- **mise**: Development tool version manager
- **just**: Command runner for task management

## Documentation

- ğŸ“– [Application Setup](./README-payload.md) - Payload CMS ecommerce template guide
- ğŸŒ± [Data Seeding](./docs/seed.md) - Generate demo data
- ğŸ”„ [Data Ingestion](./data/ingestion/README.md) - dlt pipeline configuration & Payload CMS API reference
- ğŸ”§ [Data Transformation](./data/transformation/README.md) - dbt models and usage
- ğŸ“Š [Analytics Queries](./docs/analysis_queries.md) - Sample SQL for common analyses

## Key Concepts

### Lakehouse Architecture

A **lakehouse** combines the best of data lakes and data warehouses:

- **Open formats** (Iceberg, Parquet) - no vendor lock-in
- **ACID transactions** - data consistency
- **Schema evolution** - flexible schema changes
- **Time travel** - query historical data
- **Multiple engines** - query with Trino, DuckDB, Spark, etc.

### Medallion Architecture

Data flows through three layers:

1. **Bronze** (Raw): Data from Payload CMS API â†’ Iceberg
2. **Silver** (Staging): Cleaned & standardized â†’ dbt staging views
3. **Gold** (Marts): Analytics-ready star schema â†’ dbt marts tables

### Incremental Processing

- **dlt**: Extracts only new/updated records via `updated_since` parameter
- **dbt**: Incremental models process only changed data
- **Result**: Efficient updates without full table scans

## Contributing

This is a demonstration project. Feel free to:

- Use as a reference for your own lakehouse integrations
- Adapt the dbt models for your needs
- Extend the analytics queries
- Report issues or suggest improvements

## License

See [LICENSE](./LICENSE) for details.

## Resources

- [Payload CMS](https://payloadcms.com/)
- [dlt Documentation](https://dlthub.com/docs)
- [dbt Documentation](https://docs.getdbt.com/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [Trino Documentation](https://trino.io/docs/current/)
- [buun-stack](https://github.com/buun-ch/buun-stack)
- [mise](https://mise.jdx.dev/) - Development tool version manager
- [just](https://github.com/casey/just) - Command runner
